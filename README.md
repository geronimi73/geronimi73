### üëã Hi, I‚Äôm geronimo
trying to understand LLMs. This is my journey so far:

### üöÄ Tutorials and Repositories
* A failed experiment with LISA: "Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning", [code](https://github.com/geronimi73/3090_shorts/blob/main/nb_LISA_llama2-7b.ipynb), [paper](https://arxiv.org/abs/2403.17919)
* üõ†Ô∏è [Memory-efficient LLM Training with GaLore, yet another PEFT approach](https://medium.com/@geronimo7/llm-training-on-consumer-gpus-with-galore-d25075143cfb), [code](https://github.com/geronimi73/3090_shorts/blob/main/nb_galore_llama2-7b.ipynb)
* ‚öñÔ∏è [Evaluating LLMs with Semantic Similarity](https://medium.com/@geronimo7/semscore-evaluating-llms-with-semantic-similarity-2abf5c2fadb9), [code](https://github.com/geronimi73/semscore)
* üõ†Ô∏è [Finetune TinyLlama and StableLM 2](https://medium.com/@geronimo7/tinyllama-1-1b-and-stable-lm-2-1-6b-cc0051d79be9), [code](https://github.com/geronimi73/TinyLlama-versus-StableLM2)
* üõ†Ô∏è [Finetune Microsoft's Phi-2](https://medium.com/@geronimo7/phinetuning-2-0-28a2be6de110), [code](https://github.com/geronimi73/phi2-finetune)
* üõ†Ô∏è [Finetune Mamba](https://medium.com/@geronimo7/mamba-a-shallow-dive-into-a-new-architecture-for-llms-54c70ade5957), [code](https://github.com/geronimi73/mamba/tree/main)
* üõ†Ô∏è [Finetune Llama2 and Mistral using QLoRA](https://medium.com/@geronimo7/finetuning-llama2-mistral-945f9c200611), [code](https://github.com/geronimi73/qlora-minimal)
* ‚öñÔ∏è [Evaluate LLM language capabilities with meta's Belebele benchmark](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-belebele-benchmark-91d4bbd250df), [code](https://github.com/geronimi73/belebele-llama)
* ‚öñÔ∏è [Evaluate LLM language capabilities with BLEU](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-the-bleu-score-d44c651a5e58), [code](https://github.com/geronimi73/sacrebleu-llama)
* ‚öñÔ∏è [Llama2-70B as a judge of LLMs performs almost as good as GPT-4](https://medium.com/@geronimo7/judging-the-judges-668e80f4a1f2), [code](https://github.com/geronimi73/FastChat)
* ‚öñÔ∏è [Validation loss is not a good metric for chatbot quality](https://medium.com/@geronimo7/reproducing-guanaco-141a6a85a3f7)
* ‚öñÔ∏è [Use GPT3.5 as a judge of open-source LLMs](https://medium.com/@geronimo7/open-source-chatbots-in-the-wild-9a44d7a41a48), [code](https://github.com/g588928812/FastChat_eval)
* üõ†Ô∏è [Finetune Llama on podcast transripts with QLoRA](https://medium.com/@geronimo7/from-transcripts-to-ai-chat-an-experiment-with-the-lex-fridman-podcast-3248d216ec16), [code](https://github.com/g588928812/qlora)
* üíÖ [Use Stable Diffusion for sketch-guided image generation](https://medium.com/@geronimo7/sketch-guided-stable-diffusion-a-tutorial-fb25bc69ddb5), [code](https://github.com/geronimi73/SD-minimal)

### üíé Other Repositories
* [Collection of minimal scripts for 24GB GPUs](https://github.com/geronimi73/3090_shorts)
* [Needle in a haystack test for local LLMs](https://github.com/geronimi73/LLMTest_NeedleInAHaystack)
* [FSDP multi-GPU training of Mamba](https://github.com/geronimi73/train-mamba-with-fsdp)
* [Evaluate an LLMs ability to follow instructions](https://github.com/geronimi73/instruction-eval)
* [Multi-GPU inference with ü§ó accelerate](https://github.com/geronimi73/accelerate_tricks)
* [Trial run of ReLoRA for low-resource continued pretraining of llama2-7b](https://github.com/geronimi73/ReLoRA_trial)

[Twitter](https://twitter.com/Geronimo_AI) [Hugging Face](https://huggingface.co/g-ronimo) 

  
