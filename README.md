### ğŸ‘‹ Hi, Iâ€™m geronimo
trying to understand LLMs, this is my journey so far

### ğŸš€ Tutorials and Repositories
* âš–ï¸ [Evaluating LLMs with Semantic Similarity](https://medium.com/@geronimo7/semscore-evaluating-llms-with-semantic-similarity-2abf5c2fadb9), [code](https://github.com/geronimi73/semscore)
* ğŸ› ï¸ [Finetune TinyLlama and StableLM 2](https://medium.com/@geronimo7/tinyllama-1-1b-and-stable-lm-2-1-6b-cc0051d79be9), [code](https://github.com/geronimi73/TinyLlama-versus-StableLM2)
* ğŸ› ï¸ [Finetune Microsoft's Phi-2](https://medium.com/@geronimo7/phinetuning-2-0-28a2be6de110), [code](https://github.com/geronimi73/phi2-finetune)
* ğŸ› ï¸ [Finetune Mamba](https://medium.com/@geronimo7/mamba-a-shallow-dive-into-a-new-architecture-for-llms-54c70ade5957), [code](https://github.com/geronimi73/mamba/tree/main)
* ğŸ› ï¸ [Finetune Llama2 and Mistral using QLoRA](https://medium.com/@geronimo7/finetuning-llama2-mistral-945f9c200611), [code](https://github.com/geronimi73/qlora-minimal)
* âš–ï¸ [Evaluate LLM language capabilities with meta's Belebele benchmark](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-belebele-benchmark-91d4bbd250df), [code](https://github.com/geronimi73/belebele-llama)
* âš–ï¸ [Evaluate LLM language capabilities with BLEU](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-the-bleu-score-d44c651a5e58), [code](https://github.com/geronimi73/sacrebleu-llama)
* âš–ï¸ [Llama2-70B as a judge of LLMs performs almost as good as GPT-4](https://medium.com/@geronimo7/judging-the-judges-668e80f4a1f2), [code](https://github.com/geronimi73/FastChat)
* âš–ï¸ [Validation loss is not a good metric for chatbot quality](https://medium.com/@geronimo7/reproducing-guanaco-141a6a85a3f7)
* âš–ï¸ [Use GPT3.5 as a judge of open-source LLMs](https://medium.com/@geronimo7/open-source-chatbots-in-the-wild-9a44d7a41a48), [code](https://github.com/g588928812/FastChat_eval)
* ğŸ› ï¸ [Finetune Llama on podcast transripts with QLoRA](https://medium.com/@geronimo7/from-transcripts-to-ai-chat-an-experiment-with-the-lex-fridman-podcast-3248d216ec16), [code](https://github.com/g588928812/qlora)
* ğŸ’… [Use Stable Diffusion for sketch-guided image generation](https://medium.com/@geronimo7/sketch-guided-stable-diffusion-a-tutorial-fb25bc69ddb5), [code](https://github.com/geronimi73/SD-minimal)

### ğŸ’ Other Repositories
* [Collection of minimal scripts for 24GB GPUs](https://github.com/geronimi73/3090_shorts)
* [Needle in a haystack test for local LLMs](https://github.com/geronimi73/LLMTest_NeedleInAHaystack)
* [FSDP multi-GPU training of Mamba](https://github.com/geronimi73/train-mamba-with-fsdp)
* [Evaluate an LLMs ability to follow instructions](https://github.com/geronimi73/instruction-eval)
* [Multi-GPU inference with ğŸ¤— accelerate](https://github.com/geronimi73/accelerate_tricks)
* [Trial run of ReLoRA for low-resource continued pretraining of llama2-7b](https://github.com/geronimi73/ReLoRA_trial)

[Twitter](https://twitter.com/Geronimo_AI) [Hugging Face](https://huggingface.co/g-ronimo) 

  
