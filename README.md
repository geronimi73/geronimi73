### üëã Hi, I‚Äôm geronimo, a healthcare professional with a passion for AI.

I'm trying to connect work and passion, spending my time learning everything about large language models (LLMs). 

I share my learning process as tutorials on [Medium](https://medium.com/@geronimo7), the accompanying code and any other snippets which might be useful to others can be found here.

### Tutorials and Repositories
* üõ†Ô∏è [Finetune Microsoft's Phi-2](https://medium.com/@geronimo7/phinetuning-2-0-28a2be6de110), [code](https://github.com/geronimi73/phi2-finetune)
* üõ†Ô∏è [Finetune Mamba](https://medium.com/@geronimo7/mamba-a-shallow-dive-into-a-new-architecture-for-llms-54c70ade5957), [code](https://github.com/geronimi73/mamba/tree/main)
* üõ†Ô∏è [Finetune Llama2 and Mistral using QLoRA](https://medium.com/@geronimo7/finetuning-llama2-mistral-945f9c200611), [code](https://github.com/geronimi73/qlora-minimal)
* ‚öñÔ∏è [Evaluate LLM language capabilities with meta's Belebele benchmark](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-belebele-benchmark-91d4bbd250df), [code](https://github.com/geronimi73/belebele-llama)
* ‚öñÔ∏è [Evaluate LLM language capabilities with BLEU](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-the-bleu-score-d44c651a5e58), [code](https://github.com/geronimi73/sacrebleu-llama)
* ‚öñÔ∏è [Llama2-70B as a judge of LLMs performs almost as good as GPT-4](https://medium.com/@geronimo7/judging-the-judges-668e80f4a1f2), [code](https://github.com/geronimi73/FastChat)
* ‚öñÔ∏è [Validation loss is not a good metric for chatbot quality](https://medium.com/@geronimo7/reproducing-guanaco-141a6a85a3f7)
* ‚öñÔ∏è [Use GPT3.5 as a judge of open-source LLMs](https://medium.com/@geronimo7/open-source-chatbots-in-the-wild-9a44d7a41a48), [code](https://github.com/g588928812/FastChat_eval)
* üõ†Ô∏è [Finetune Llama on podcast transripts with QLoRA](https://medium.com/@geronimo7/from-transcripts-to-ai-chat-an-experiment-with-the-lex-fridman-podcast-3248d216ec16), [code](https://github.com/g588928812/qlora)
* üíÖ [Use Stable Diffusion for sketch-guided image generation](https://medium.com/@geronimo7/sketch-guided-stable-diffusion-a-tutorial-fb25bc69ddb5), [code](https://github.com/geronimi73/SD-minimal)

### Other Repositories
* [Needle in a haystack test for local LLMs](https://github.com/geronimi73/LLMTest_NeedleInAHaystack)
* [FSDP multi-GPU training of Mamba](https://github.com/geronimi73/train-mamba-with-fsdp)
* [Evaluate an LLMs ability to follow instructions](https://github.com/geronimi73/instruction-eval)
* [Multi-GPU inference with ü§ó accelerate](https://github.com/geronimi73/accelerate_tricks)
* [Trial run of ReLoRA for low-resource continued pretraining of llama2-7b](https://github.com/geronimi73/ReLoRA_trial)

### üì´ How to reach me
[Twitter](https://twitter.com/Geronimo_AI) [Discord](https://discordapp.com/users/geronimi73/) [Hugging Face](https://huggingface.co/g-ronimo) 

  
