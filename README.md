### 👋 Hi, I’m geronimo
trying to understand LLMs, this is my journey so far

### 🚀 Tutorials and Repositories
* ⚖️ [Evaluating LLMs with Semantic Similarity](https://medium.com/@geronimo7/semscore-evaluating-llms-with-semantic-similarity-2abf5c2fadb9), [code](https://github.com/geronimi73/semscore)
* 🛠️ [Finetune TinyLlama and StableLM 2](https://medium.com/@geronimo7/tinyllama-1-1b-and-stable-lm-2-1-6b-cc0051d79be9), [code](https://github.com/geronimi73/TinyLlama-versus-StableLM2)
* 🛠️ [Finetune Microsoft's Phi-2](https://medium.com/@geronimo7/phinetuning-2-0-28a2be6de110), [code](https://github.com/geronimi73/phi2-finetune)
* 🛠️ [Finetune Mamba](https://medium.com/@geronimo7/mamba-a-shallow-dive-into-a-new-architecture-for-llms-54c70ade5957), [code](https://github.com/geronimi73/mamba/tree/main)
* 🛠️ [Finetune Llama2 and Mistral using QLoRA](https://medium.com/@geronimo7/finetuning-llama2-mistral-945f9c200611), [code](https://github.com/geronimi73/qlora-minimal)
* ⚖️ [Evaluate LLM language capabilities with meta's Belebele benchmark](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-belebele-benchmark-91d4bbd250df), [code](https://github.com/geronimi73/belebele-llama)
* ⚖️ [Evaluate LLM language capabilities with BLEU](https://medium.com/@geronimo7/evaluating-language-competence-of-llama-2-based-models-the-bleu-score-d44c651a5e58), [code](https://github.com/geronimi73/sacrebleu-llama)
* ⚖️ [Llama2-70B as a judge of LLMs performs almost as good as GPT-4](https://medium.com/@geronimo7/judging-the-judges-668e80f4a1f2), [code](https://github.com/geronimi73/FastChat)
* ⚖️ [Validation loss is not a good metric for chatbot quality](https://medium.com/@geronimo7/reproducing-guanaco-141a6a85a3f7)
* ⚖️ [Use GPT3.5 as a judge of open-source LLMs](https://medium.com/@geronimo7/open-source-chatbots-in-the-wild-9a44d7a41a48), [code](https://github.com/g588928812/FastChat_eval)
* 🛠️ [Finetune Llama on podcast transripts with QLoRA](https://medium.com/@geronimo7/from-transcripts-to-ai-chat-an-experiment-with-the-lex-fridman-podcast-3248d216ec16), [code](https://github.com/g588928812/qlora)
* 💅 [Use Stable Diffusion for sketch-guided image generation](https://medium.com/@geronimo7/sketch-guided-stable-diffusion-a-tutorial-fb25bc69ddb5), [code](https://github.com/geronimi73/SD-minimal)

### 💎 Other Repositories
* [Collection of minimal scripts for 24GB GPUs](https://github.com/geronimi73/3090_shorts)
* [Needle in a haystack test for local LLMs](https://github.com/geronimi73/LLMTest_NeedleInAHaystack)
* [FSDP multi-GPU training of Mamba](https://github.com/geronimi73/train-mamba-with-fsdp)
* [Evaluate an LLMs ability to follow instructions](https://github.com/geronimi73/instruction-eval)
* [Multi-GPU inference with 🤗 accelerate](https://github.com/geronimi73/accelerate_tricks)
* [Trial run of ReLoRA for low-resource continued pretraining of llama2-7b](https://github.com/geronimi73/ReLoRA_trial)

[Twitter](https://twitter.com/Geronimo_AI) [Hugging Face](https://huggingface.co/g-ronimo) 

  
